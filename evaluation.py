# evaluation.py

import json
import os
import numpy as np
from tqdm import tqdm
# Ensure sentence_transformers is in your requirements.txt
from sentence_transformers import SentenceTransformer, util
from pydantic import BaseModel, Field
from typing import Dict, Any, Union, Optional

import config
from llm_utils import call_api_client
from counselor_models import (
    run_srs_reflector, MemoryManager,
    LocalAdaptiveCounselor, ClosedAdaptiveCounselor,
    LocalBaselineCounselor, LocalBaselineNoMemoryCounselor,
    ClosedBaselineCounselor, ClosedBaselineNoMemoryCounselor, ClinicalSummary,
    append_to_log
)

# Schemas (Scores, TascRubricScores) remain the same


class Scores(BaseModel):
    task_alignment: int = Field(alias='T')
    alliance_bond: int = Field(alias='A')
    stylistic_congruence: int = Field(alias='S')
    congruence_with_goals: int = Field(alias='C')
    overall_score: float

    class Config:
        populate_by_name = True


class TascRubricScores(BaseModel):
    justification: Union[str, Dict[str, Any]]
    scores: Scores


def calculate_synthesis_accuracy(predicted_profile_dict: Optional[Dict[str, Any]], ground_truth_summary_dict: Optional[Dict[str, Any]]) -> float:
    if not predicted_profile_dict or not ground_truth_summary_dict:
        # print("Warning: Missing predicted or ground truth profile for synthesis accuracy.")
        return 0.0

    model = SentenceTransformer('all-MiniLM-L6-v2')

    # For ClinicalSummary, we might compare a concatenation of key fields
    # or focus on one like 'plan_for_next_session' if there's a ground truth for it.
    # Let's create comparable strings from key fields.
    def extract_comparable_text(summary_dict: Optional[Dict[str, Any]]) -> str:
        if not summary_dict:
            return ""
        texts = [
            summary_dict.get('session_focus', ''),
            summary_dict.get('user_emotional_state', ''),
            " ".join(summary_dict.get('therapeutic_milestones', [])),
            " ".join(summary_dict.get('emerging_themes', [])),
            summary_dict.get('plan_for_next_session', ''),
            summary_dict.get('counselor_reflections_on_session', '')
        ]
        return " ".join(filter(None, texts)).strip()

    # The 'predicted_profile' passed to this function is the .model_dump() of ClinicalSummary
    # The 'ground_truth_profile' in the main loop refers to case['sessions'][i]['summary']
    # which is also a .model_dump() of a ClinicalSummary (the one from the previous Reflector pass).
    # This means we are comparing a Reflector's output against the previous Reflector's output.
    # This measures consistency or evolution of the summary itself.

    gt_text = extract_comparable_text(ground_truth_summary_dict)
    pred_text = extract_comparable_text(predicted_profile_dict)

    if not gt_text or not pred_text:
        # print(f"Warning: Empty comparable text for synthesis accuracy. GT: '{gt_text}', Pred: '{pred_text}'")
        return 0.0

    embedding1 = model.encode(gt_text, convert_to_tensor=True)
    embedding2 = model.encode(pred_text, convert_to_tensor=True)
    similarity = util.pytorch_cos_sim(embedding1, embedding2).item()
    return max(0, similarity)


def run_tasc_evaluation(case_data: Dict, counselor_response: str, log_file_path: str) -> TascRubricScores:
    append_to_log(log_file_path, "\n--- T.A.S.C. Evaluation Call ---")
    final_session_plan = "Not available"
    preferred_style = "general supportive"  # Default
    evolved_goals_str = "general therapeutic progress"  # Default

    # Extract context for the judge LLM from the case_data
    # The 'summary' field in session 5 (index 4) holds the ClinicalSummary dict generated by the reflector
    if len(case_data['sessions']) >= 5 and 'summary' in case_data['sessions'][4] and case_data['sessions'][4]['summary']:
        last_summary = case_data['sessions'][4]['summary']
        if isinstance(last_summary, dict):
            final_session_plan = last_summary.get(
                'plan_for_next_session', "Not available")
            # Evolved goals could be a combination of milestones and emerging themes
            evolved_goals_list = last_summary.get(
                'therapeutic_milestones', []) + last_summary.get('emerging_themes', [])
            if evolved_goals_list:
                evolved_goals_str = ", ".join(evolved_goals_list)

    # Extract initial preferred_style. This relies on dataset generation correctly placing it
    # or the first ClinicalSummary capturing it. For this example, we'll make it more robust.
    # The initial dataset 'generate_dataset.py' does not add 'initial_profile' or 'evolved_profile' keys to the case.
    # We derive these from the first and last summaries if needed by the judge.
    # For stylistic congruence, we need 'preferred_style'. This should ideally be in the *dataset's* initial profile.
    # Let's assume the first SRS summary might capture it in 'session_focus' or 'emerging_themes' or we make a placeholder.
    if case_data['sessions'] and 'summary' in case_data['sessions'][0] and case_data['sessions'][0]['summary']:
        first_summary = case_data['sessions'][0]['summary']
        if isinstance(first_summary, dict):
            # This is a heuristic; 'preferred_style' should ideally be a dedicated field from session 1 data.
            # Or another field if it was captured
            preferred_style = first_summary.get(
                'session_focus', preferred_style)

    prompt = f"""
    You are an expert evaluator of AI therapeutic systems. Score a response using the T.A.S.C. rubric.

    **GROUND TRUTH CONTEXT:**
    - **User's Agreed Plan (from end of Session 5):** "{final_session_plan}"
    - **User's Evolved Goals (synthesized from Session 5 summary):** "{evolved_goals_str}"
    - **User's Preferred Style (approximated from early session focus/themes):** "{preferred_style}"
    - **Final User Turn (The Test Probe from Session 6):** "{case_data['sessions'][-1]['transcript'][-1]['content']}"

    **COUNSELOR RESPONSE TO EVALUATE:**
    "{counselor_response}"

    **T.A.S.C. RUBRIC (Score each 1-5):**
    1.  **T - Task Alignment**: The user's agreed plan was "{final_session_plan}". How well does the response guide the user through this specific task/plan?
        - 5 (Excellent): Masterfully implements the task, weaving it seamlessly into the conversation.
        - 4 (Good): Correctly and clearly implements the task as requested.
        - 3 (Moderate): Attempts to implement the task, but the execution is flawed, incomplete, or slightly misaligned.
        - 2 (Weak): Barely mentions the task or alludes to it vaguely without any real implementation.
        - 1 (Poor): Completely ignores or contradicts the plan.
    2.  **A - Alliance Bond**:
        - 5 (Excellent): Deeply empathetic; not only validates the user's feelings but also reflects their underlying meaning, significantly strengthening the alliance.
        - 4 (Good): Clearly validates the user's specific emotions and perspective, making them feel heard and understood.
        - 3 (Moderate): Shows basic empathy (e.g., "I understand," "That sounds hard") but lacks specificity.
        - 2 (Weak): The response is polite but emotionally detached or uses generic, unhelpful platitudes.
        - 1 (Poor): The response is robotic, dismissive, or invalidating.
    3.  **S - Stylistic Congruence**: The user's approximated preferred style is "{preferred_style}". How well does the response's language match this style (or a general supportive, professional counseling style if preference is unclear)?
        - 5 (Excellent): Perfectly embodies the preferred/supportive style, making the interaction feel naturally and precisely tailored.
        - 4 (Good): Consistently and clearly matches the preferred/supportive style.
        - 3 (Moderate): Attempts the style but may be inconsistent or awkward.
        - 2 (Weak): The style is generic and lacks therapeutic quality.
        - 1 (Poor): The style is inappropriate or counter-therapeutic.
    4.  **C - Congruence with Goals**: The user's synthesized evolved goals are related to "{evolved_goals_str}". Does the response help the user move toward these goals or general therapeutic progress?
        - 5 (Excellent): Masterfully connects the task to these evolved goals or implicit therapeutic goals like self-understanding.
        - 4 (Good): The response clearly helps the user move toward their evolved goals or general therapeutic progress.
        - 3 (Moderate): The response is generally helpful but not explicitly goal-directed towards these evolved goals.
        - 2 (Weak): The connection to these goals is tangential.
        - 1 (Poor): The response is irrelevant or counter-productive to these goals.

    **TASK:**
    Provide your analysis in a single JSON object with two top-level keys: "justification" and "scores".
    1.  `justification`: A string or a dictionary explaining your reasoning for the scores for T, A, S, and C.
    2.  `scores`: A NESTED JSON object containing your scores for "T", "A", "S", "C", and the calculated "overall_score" (the average of the four scores). Ensure the keys in this nested object are "T", "A", "S", "C", and "overall_score".
    """

    class ExpectedJudgeResponse(BaseModel):
        justification: Union[str, Dict[str, Any]]
        scores: Scores

    judge_response_obj = call_api_client(
        prompt, ExpectedJudgeResponse, model=config.JUDGE_MODEL, temperature=0.0)

    if not judge_response_obj:
        append_to_log(
            log_file_path, "T.A.S.C. Evaluation FAILED for this response (Judge LLM call failed).")
        default_scores_sub = Scores(T=0, A=0, S=0, C=0, overall_score=0.0)
        return TascRubricScores(justification="Judge LLM call failed or returned invalid format.", scores=default_scores_sub)

    append_to_log(
        log_file_path, f"T.A.S.C. Raw Scores: {json.dumps(judge_response_obj.model_dump(by_alias=True), indent=2)}")
    return TascRubricScores(justification=judge_response_obj.justification, scores=judge_response_obj.scores)


def main():
    DATASET_PATH = os.path.join("output", "counseling_dataset_6sessions.json")
    RESULTS_PATH = os.path.join(
        "output", "evaluation_results_full_cohort.json")

    os.makedirs(config.LOG_DIR, exist_ok=True)

    if not os.path.exists(DATASET_PATH):
        print(
            f"Dataset not found at {DATASET_PATH}. Please run `generate_dataset.py` first.")
        return

    with open(DATASET_PATH, 'r') as f:
        dataset = json.load(f)

    memory_manager = MemoryManager()

    counselors = {
        "local_adaptive": LocalAdaptiveCounselor(config.LOCAL_MODEL_NAME, memory_manager.memory, "_adaptive"),
        # ADDED
        "closed_adaptive": ClosedAdaptiveCounselor(config.CLOSED_MODEL_NAME, memory_manager.memory, "_adaptive"),
        "local_baseline": LocalBaselineCounselor(config.LOCAL_MODEL_NAME, memory_manager.memory, "_baseline_local"),
        "local_no_memory": LocalBaselineNoMemoryCounselor(config.LOCAL_MODEL_NAME),
        "closed_baseline": ClosedBaselineCounselor(config.CLOSED_MODEL_NAME, memory_manager.memory, "_baseline_closed"),
        "closed_no_memory": ClosedBaselineNoMemoryCounselor(config.CLOSED_MODEL_NAME),
    }

    all_results = []
    print(f"Starting evaluation for {len(dataset)} cases...")

    for case_idx, case in enumerate(tqdm(dataset, desc="Processing Cases")):
        case_id = case['case_id']

        case_log_dir = os.path.join(config.LOG_DIR, case_id)
        os.makedirs(case_log_dir, exist_ok=True)

        print(f"\n--- Simulating Case: {case_id} ---")
        case_progress_log_file = os.path.join(
            case_log_dir, "_case_simulation_log.txt")
        append_to_log(case_progress_log_file,
                      f"--- Simulating Case: {case_id} Start ---")

        memory_manager.clear_user_history(case_id)
        append_to_log(case_progress_log_file,
                      "Cleared user history for all tracks.")

        previous_summary_dict: Optional[Dict] = None
        for i in range(5):
            session = case['sessions'][i]
            session_num_for_log = session['session_number']
            print(f"  - Ingesting Session {session_num_for_log} for memory...")
            append_to_log(case_progress_log_file,
                          f"  - Ingesting Session {session_num_for_log} for memory...")

            memory_manager.add_raw_turns_for_baseline(
                case_id, session['transcript'], "baseline_local")
            memory_manager.add_raw_turns_for_baseline(
                case_id, session['transcript'], "baseline_closed")

            summary_obj: Optional[ClinicalSummary] = run_srs_reflector(
                session['transcript'],
                # Pass as ClinicalSummary obj
                ClinicalSummary.model_validate(
                    previous_summary_dict) if previous_summary_dict else None
            )
            if summary_obj:
                memory_manager.add_srs_summary_for_adaptive(
                    case_id, summary_obj, session_num_for_log)
                # Store the summary in case_data for the judge's context (for all models)
                case['sessions'][i]['summary'] = summary_obj.model_dump()
                previous_summary_dict = summary_obj.model_dump()
                append_to_log(
                    case_progress_log_file, f"    SRS Summary for Session {session_num_for_log} generated and stored for _adaptive track.")
            elif i > 0 and ('summary' not in case['sessions'][i] or not case['sessions'][i]['summary']):
                case['sessions'][i]['summary'] = previous_summary_dict if previous_summary_dict else {}
                append_to_log(
                    case_progress_log_file, f"    SRS Reflector FAILED for Session {session_num_for_log}. Using previous summary for context if available.")
            elif i == 0 and ('summary' not in case['sessions'][i] or not case['sessions'][i]['summary']):
                case['sessions'][i]['summary'] = {}

        print(f"  --- Memory Ingestion Complete for {case_id} ---")
        append_to_log(case_progress_log_file,
                      "--- Memory Ingestion Complete ---")

        test_probe = case['sessions'][5]['transcript'][-1]['content']
        case_results_data = {"case_id": case_id, "models": {}}

        for name, model_instance in counselors.items():
            model_log_dir = os.path.join(case_log_dir, name)
            os.makedirs(model_log_dir, exist_ok=True)
            log_file_path = os.path.join(model_log_dir, "interaction_log.txt")

            print(f"  - Evaluating model: {name}...")
            append_to_log(case_progress_log_file,
                          f"  - Evaluating model: {name}...")

            response_str = model_instance.get_response(
                user_id=case_id, case_data=case, test_probe=test_probe, log_file_path=log_file_path
            )

            error_signature = "Error: Could not generate response."
            if error_signature in response_str:
                append_to_log(
                    log_file_path, f"Model {name} returned an error: {response_str}")
                failed_scores_sub = Scores(
                    T=0, A=0, S=0, C=0, overall_score=0.0)
                scores_obj = TascRubricScores(
                    justification=f"Model {name} failed to generate a valid response.",
                    scores=failed_scores_sub
                )
            else:
                scores_obj = run_tasc_evaluation(
                    case, response_str, log_file_path)

            case_results_data["models"][name] = {
                "response": response_str,
                "scores_data": scores_obj.model_dump(by_alias=True)
            }

        all_results.append(case_results_data)
        append_to_log(case_progress_log_file,
                      f"--- Simulating Case: {case_id} End ---")

    with open(RESULTS_PATH, 'w') as f:
        json.dump(all_results, f, indent=2)
    print(f"\nDetailed results saved to {RESULTS_PATH}")

    print("\n\n--- FINAL EVALUATION REPORT ---")
    if not all_results:
        print("No results to report.")
        return

    model_names = list(counselors.keys())
    metrics_map_for_report = [
        ("Task Alignment", "T"), ("Alliance Bond", "A"),
        ("Stylistic Congruence", "S"), ("Congruence With Goals", "C"),
        ("Overall Score", "overall_score")
    ]
    avg_scores_report = {}

    for model_name in model_names:
        avg_scores_report[model_name] = {}
        for display_name, key_in_json in metrics_map_for_report:
            metric_data = []
            for r_val in all_results:
                try:
                    model_result = r_val.get('models', {}).get(model_name, {})
                    scores_data = model_result.get('scores_data', {})
                    nested_scores = scores_data.get('scores', {})

                    score_value = nested_scores.get(key_in_json)
                    if score_value is not None:
                        metric_data.append(float(score_value))
                    else:
                        metric_data.append(0.0)
                except Exception as e:
                    metric_data.append(0.0)

            avg_scores_report[model_name][key_in_json] = np.mean(
                metric_data) if metric_data else 0.0

    print("\nAverage Overall T.A.S.C. Scores:")
    for model_name in model_names:
        print(
            f"  - {model_name:<30}: {avg_scores_report[model_name]['overall_score']:.3f}")

    print("\nDetailed Average T.A.S.C. Score Breakdown:")
    header = f"{'Metric':<35}"
    for model_name in model_names:
        header += f" | {model_name[:15]:<15}"
    print(header)
    print("-" * len(header))

    for display_name, key_in_json in metrics_map_for_report[:-1]:
        row = f"{display_name:<35}"
        for model_name in model_names:
            row += f" | {avg_scores_report[model_name][key_in_json]:<15.3f}"
        print(row)


if __name__ == "__main__":
    main()
